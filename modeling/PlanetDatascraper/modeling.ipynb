{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1592e6340a55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mset_random_seed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mset_random_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# Borrowed Neural Net\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1)\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Concatenate, Dropout, regularizers, Cropping2D, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam, Nadam\n",
    "from keras.utils.training_utils import multi_gpu_model\n",
    "from ..utils import get_model_name\n",
    "from src.models.model_utils import jaccard_coef, jaccard_coef_thresholded, jaccard_coef_loss, swish, get_callbacks, ImageSequence\n",
    "from keras.utils.generic_utils import get_custom_objects  # To use swish activation function\n",
    "\n",
    "\n",
    "class Unet(object):\n",
    "    def __init__(self, params):\n",
    "        # Seed for the random generators\n",
    "        self.seed = 1\n",
    "\n",
    "        # Find the model you would like\n",
    "        model_name = get_model_name(params)\n",
    "\n",
    "        # Find the number of classes and bands\n",
    "        if params.collapse_cls:\n",
    "            n_cls = 1\n",
    "        else:\n",
    "            n_cls = np.size(params.cls)\n",
    "        n_bands = np.size(params.bands)\n",
    "\n",
    "        # Create the model in keras\n",
    "        if params.num_gpus == 1:\n",
    "            self.model = self.__create_inference__(n_bands, n_cls, params)  # initialize the model\n",
    "            try:\n",
    "                self.model.load_weights(params.project_path + 'models/Unet/' + model_name)\n",
    "                print('Weights loaded from model: ' + model_name)\n",
    "            except:\n",
    "                print('No weights found')\n",
    "\n",
    "        else:\n",
    "            with tf.device(\"/cpu:0\"):\n",
    "                self.model = self.__create_inference__(n_bands, n_cls, params)  # initialize the model on the CPU\n",
    "                try:\n",
    "                    self.model.load_weights(params.project_path + 'models/Unet/' + model_name)\n",
    "                    print('Weights loaded from model: ' + model_name)\n",
    "                except:\n",
    "                    print('No weights found')\n",
    "            self.model = multi_gpu_model(self.model, gpus=params.num_gpus)  # Make it run on multiple GPUs\n",
    "\n",
    "    def __create_inference__(self, n_bands, n_cls, params):\n",
    "        # Note about BN and dropout: https://stackoverflow.com/questions/46316687/how-to-include-batch-normalization-in-non-sequential-keras-model\n",
    "        get_custom_objects().update({'swish': Activation(swish)})\n",
    "        inputs = Input((params.patch_size, params.patch_size, n_bands))\n",
    "        # -----------------------------------------------------------------------\n",
    "        conv1 = Conv2D(32, (3, 3), activation=params.activation_func, padding='same',\n",
    "                       kernel_regularizer=regularizers.l2(params.L2reg))(inputs)\n",
    "        conv1 = BatchNormalization(momentum=params.batch_norm_momentum)(conv1) if params.use_batch_norm else conv1\n",
    "        conv1 = Conv2D(32, (3, 3), activation=params.activation_func, padding='same',\n",
    "                       kernel_regularizer=regularizers.l2(params.L2reg))(conv1)\n",
    "        conv1 = BatchNormalization(momentum=params.batch_norm_momentum)(conv1) if params.use_batch_norm else conv1\n",
    "        pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "        # -----------------------------------------------------------------------\n",
    "        conv2 = Conv2D(64, (3, 3), activation=params.activation_func, padding='same',\n",
    "                       kernel_regularizer=regularizers.l2(params.L2reg))(pool1)\n",
    "        conv2 = BatchNormalization(momentum=params.batch_norm_momentum)(conv2) if params.use_batch_norm else conv2\n",
    "        conv2 = Conv2D(64, (3, 3), activation=params.activation_func, padding='same',\n",
    "                       kernel_regularizer=regularizers.l2(params.L2reg))(conv2)\n",
    "        conv2 = BatchNormalization(momentum=params.batch_norm_momentum)(conv2) if params.use_batch_norm else conv2\n",
    "        pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "        # -----------------------------------------------------------------------\n",
    "        conv3 = Conv2D(128, (3, 3), activation=params.activation_func, padding='same',\n",
    "                       kernel_regularizer=regularizers.l2(params.L2reg))(pool2)\n",
    "        conv3 = BatchNormalization(momentum=params.batch_norm_momentum)(conv3) if params.use_batch_norm else conv3\n",
    "        conv3 = Conv2D(128, (3, 3), activation=params.activation_func, padding='same',\n",
    "                       kernel_regularizer=regularizers.l2(params.L2reg))(conv3)\n",
    "        conv3 = BatchNormalization(momentum=params.batch_norm_momentum)(conv3) if params.use_batch_norm else conv3\n",
    "        pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "        # -----------------------------------------------------------------------\n",
    "        conv4 = Conv2D(256, (3, 3), activation=params.activation_func, padding='same',\n",
    "                       kernel_regularizer=regularizers.l2(params.L2reg))(pool3)\n",
    "        conv4 = BatchNormalization(momentum=params.batch_norm_momentum)(conv4) if params.use_batch_norm else conv4\n",
    "        conv4 = Conv2D(256, (3, 3), activation=params.activation_func, padding='same',\n",
    "                       kernel_regularizer=regularizers.l2(params.L2reg))(conv4)\n",
    "        conv4 = BatchNormalization(momentum=params.batch_norm_momentum)(conv4) if params.use_batch_norm else conv4\n",
    "        pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "        # -----------------------------------------------------------------------\n",
    "        conv5 = Conv2D(512, (3, 3), activation=params.activation_func, padding='same',\n",
    "                       kernel_regularizer=regularizers.l2(params.L2reg))(pool4)\n",
    "        conv5 = BatchNormalization(momentum=params.batch_norm_momentum)(conv5) if params.use_batch_norm else conv5\n",
    "        conv5 = Conv2D(512, (3, 3), activation=params.activation_func, padding='same',\n",
    "                       kernel_regularizer=regularizers.l2(params.L2reg))(conv5)\n",
    "        conv5 = BatchNormalization(momentum=params.batch_norm_momentum)(conv5) if params.use_batch_norm else conv5\n",
    "        # -----------------------------------------------------------------------\n",
    "        up6 = Concatenate(axis=3)([UpSampling2D(size=(2, 2))(conv5), conv4])\n",
    "        conv6 = Conv2D(256, (3, 3), activation=params.activation_func, padding='same',\n",
    "                       kernel_regularizer=regularizers.l2(params.L2reg))(up6)\n",
    "        conv6 = Dropout(params.dropout)(conv6) if not params.dropout_on_last_layer_only else conv6\n",
    "        conv6 = Conv2D(256, (3, 3), activation=params.activation_func, padding='same',\n",
    "                       kernel_regularizer=regularizers.l2(params.L2reg))(conv6)\n",
    "        conv6 = Dropout(params.dropout)(conv6) if not params.dropout_on_last_layer_only else conv6\n",
    "        # -----------------------------------------------------------------------\n",
    "        up7 = Concatenate(axis=3)([UpSampling2D(size=(2, 2))(conv6), conv3])\n",
    "        conv7 = Conv2D(128, (3, 3), activation=params.activation_func, padding='same',\n",
    "                       kernel_regularizer=regularizers.l2(params.L2reg))(up7)\n",
    "        conv7 = Dropout(params.dropout)(conv7) if not params.dropout_on_last_layer_only else conv7\n",
    "        conv7 = Conv2D(128, (3, 3), activation=params.activation_func, padding='same',\n",
    "                       kernel_regularizer=regularizers.l2(params.L2reg))(conv7)\n",
    "        conv7 = Dropout(params.dropout)(conv7) if not params.dropout_on_last_layer_only else conv7\n",
    "        # -----------------------------------------------------------------------\n",
    "        up8 = Concatenate(axis=3)([UpSampling2D(size=(2, 2))(conv7), conv2])\n",
    "        conv8 = Conv2D(64, (3, 3), activation=params.activation_func, padding='same',\n",
    "                       kernel_regularizer=regularizers.l2(params.L2reg))(up8)\n",
    "        conv8 = Dropout(params.dropout)(conv8) if not params.dropout_on_last_layer_only else conv8\n",
    "        conv8 = Conv2D(64, (3, 3), activation=params.activation_func, padding='same',\n",
    "                       kernel_regularizer=regularizers.l2(params.L2reg))(conv8)\n",
    "        conv8 = Dropout(params.dropout)(conv8) if not params.dropout_on_last_layer_only else conv8\n",
    "        # -----------------------------------------------------------------------\n",
    "        up9 = Concatenate(axis=3)([UpSampling2D(size=(2, 2))(conv8), conv1])\n",
    "        conv9 = Conv2D(32, (3, 3), activation=params.activation_func, padding='same',\n",
    "                       kernel_regularizer=regularizers.l2(params.L2reg))(up9)\n",
    "        conv9 = Dropout(params.dropout)(conv9) if not params.dropout_on_last_layer_only else conv9\n",
    "        conv9 = Conv2D(32, (3, 3), activation=params.activation_func, padding='same',\n",
    "                       kernel_regularizer=regularizers.l2(params.L2reg))(conv9)\n",
    "        conv9 = Dropout(params.dropout)(conv9)\n",
    "        # -----------------------------------------------------------------------\n",
    "        clip_pixels = np.int32(params.overlap / 2)  # Only used for input in Cropping2D function on next line\n",
    "        crop9 = Cropping2D(cropping=((clip_pixels, clip_pixels), (clip_pixels, clip_pixels)))(conv9)\n",
    "        # -----------------------------------------------------------------------\n",
    "        conv10 = Conv2D(n_cls, (1, 1), activation='sigmoid')(crop9)\n",
    "        # -----------------------------------------------------------------------\n",
    "        model = Model(inputs=inputs, outputs=conv10)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def train(self, params):\n",
    "        # Define callbacks\n",
    "        csv_logger, model_checkpoint, reduce_lr, tensorboard, early_stopping = get_callbacks(params)\n",
    "        used_callbacks = [csv_logger, model_checkpoint, tensorboard]\n",
    "        if params.reduce_lr:\n",
    "            used_callbacks.append(reduce_lr)\n",
    "        if params.early_stopping:\n",
    "            used_callbacks.append(early_stopping)\n",
    "\n",
    "        # Configure optimizer (use Nadam or Adam and 'binary_crossentropy' or jaccard_coef_loss)\n",
    "        if params.optimizer == 'Adam':\n",
    "            if params.loss_func == 'binary_crossentropy':\n",
    "                self.model.compile(optimizer=Adam(lr=params.learning_rate, decay=params.decay, amsgrad=True),\n",
    "                                   loss='binary_crossentropy',\n",
    "                                   metrics=['binary_crossentropy', jaccard_coef_loss, jaccard_coef,\n",
    "                                            jaccard_coef_thresholded, 'accuracy'])\n",
    "            elif params.loss_func == 'jaccard_coef_loss':\n",
    "                self.model.compile(optimizer=Adam(lr=params.learning_rate, decay=params.decay, amsgrad=True),\n",
    "                                   loss=jaccard_coef_loss,\n",
    "                                   metrics=['binary_crossentropy', jaccard_coef_loss, jaccard_coef,\n",
    "                                            jaccard_coef_thresholded, 'accuracy'])\n",
    "        elif params.optimizer == 'Nadam':\n",
    "            if params.loss_func == 'binary_crossentropy':\n",
    "                self.model.compile(optimizer=Nadam(lr=params.learning_rate),\n",
    "                                   loss='binary_crossentropy',\n",
    "                                   metrics=['binary_crossentropy', jaccard_coef_loss, jaccard_coef,\n",
    "                                            jaccard_coef_thresholded, 'accuracy'])\n",
    "            elif params.loss_func == 'jaccard_coef_loss':\n",
    "                self.model.compile(optimizer=Nadam(lr=params.learning_rate),\n",
    "                                   loss=jaccard_coef_loss,\n",
    "                                   metrics=['binary_crossentropy', jaccard_coef_loss, jaccard_coef,\n",
    "                                            jaccard_coef_thresholded, 'accuracy'])\n",
    "\n",
    "        # Create generators\n",
    "        image_generator = ImageSequence(params, shuffle=True, seed=self.seed, augment_data=params.affine_transformation)\n",
    "        val_generator = ImageSequence(params, shuffle=True, seed=self.seed, augment_data=params.affine_transformation,\n",
    "                                      validation_generator=True)\n",
    "\n",
    "        # Do the training\n",
    "        print('------------------------------------------')\n",
    "        print('Start training:')\n",
    "        self.model.fit_generator(image_generator,\n",
    "                                 epochs=params.epochs,\n",
    "                                 steps_per_epoch=params.steps_per_epoch,\n",
    "                                 verbose=1,\n",
    "                                 workers=4,\n",
    "                                 max_queue_size=16,\n",
    "                                 use_multiprocessing=True,\n",
    "                                 shuffle=False,\n",
    "                                 callbacks=used_callbacks,\n",
    "                                 validation_data=val_generator,\n",
    "                                 validation_steps=None)\n",
    "\n",
    "        # Save the weights (append the val score in the name)\n",
    "        # There is a bug with multi_gpu_model (https://github.com/kuza55/keras-extras/issues/3), hence model.layers[-2]\n",
    "        model_name = get_model_name(params)\n",
    "        if params.num_gpus != 1:\n",
    "            self.model = self.model.layers[-2]\n",
    "            self.model.save_weights(params.project_path + 'models/Unet/' + model_name)\n",
    "            self.model = multi_gpu_model(self.model, gpus=params.num_gpus)  # Make it run on multiple GPUs\n",
    "        else:\n",
    "            self.model.save_weights(params.project_path + 'models/Unet/' + model_name)\n",
    "\n",
    "    def predict(self, img, n_bands, n_cls, num_gpus, params):\n",
    "        # Predict batches of patches\n",
    "        patches = np.shape(img)[0]  # Total number of patches\n",
    "        patch_batch_size = 128\n",
    "\n",
    "        # Do the prediction\n",
    "        predicted = np.zeros((patches, params.patch_size - params.overlap, params.patch_size - params.overlap, n_cls))\n",
    "        for i in range(0, patches, patch_batch_size):\n",
    "            predicted[i:i + patch_batch_size, :, :, :] = self.model.predict(img[i:i + patch_batch_size, :, :, :])\n",
    "\n",
    "        return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
